{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bebfd0",
   "metadata": {
    "id": "97bebfd0"
   },
   "source": [
    "## **1. Dataset 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52703e1b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1620908248591,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "52703e1b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = DataFrame(pd.read_csv('creditcard.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d549f39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "5d549f39",
    "outputId": "5b7ac8df-de9e-4b5f-d0c4-84a8b21143f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# df.describe()\n",
    "# df.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fed5c7",
   "metadata": {},
   "source": [
    "### * **class 값 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb10e7fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb10e7fa",
    "outputId": "da4d39cd-e61a-4552-9894-a4eab3f9474e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dateset\n",
      "Frauds 0.17 %of the dateset\n"
     ]
    }
   ],
   "source": [
    "print(\"No Frauds\", round(df[\"Class\"].value_counts()[0]/len(df) * 100,2),\n",
    "     \"%of the dateset\")\n",
    "print(\"Frauds\", round(df[\"Class\"].value_counts()[1]/len(df) * 100,2),\n",
    "     \"%of the dateset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded9ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81adf61f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1620908249134,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "81adf61f",
    "outputId": "e396d059-7ba0-4373-ea0d-ef8d3032e50a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#4374D9\", \"#F361A6\"]\n",
    "\n",
    "sns.countplot(\"Class\", data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65676a4a",
   "metadata": {},
   "source": [
    "### * **StandardScaler Amount 값 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59056a37",
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1620908249476,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "59056a37"
   },
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "amount = dataset[:,29]\n",
    "# 정규화를 위한 차원변경\n",
    "amount = amount.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eadfbf2",
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1620908249854,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "4eadfbf2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# StandardScaler() 정규화\n",
    "standardScaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "amount_data_standard = standardScaler.fit(amount).transform(amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb26684",
   "metadata": {},
   "source": [
    "### * **Time 데이터 정규화 방법 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46507aba",
   "metadata": {
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1620908250459,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "46507aba"
   },
   "outputs": [],
   "source": [
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df['loged_time'] = np.log1p(df['Time'].values.reshape(-1,1))\n",
    "df['std_time'] = standardScaler.fit_transform(df['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de12ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 5625,
     "status": "ok",
     "timestamp": 1620908255876,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "9de12ec5",
    "outputId": "fd5b6ca2-0218-4271-e5e6-db60f6f789d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.996583023457193, 1.6420577336572635)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(18,4))\n",
    "scaled_time = df['scaled_time'].values\n",
    "loged_time = df['loged_time'].values\n",
    "std_time = df['std_time'].values\n",
    "\n",
    "sns.distplot(scaled_time, ax=ax[0], color='r', axlabel=\"RobustScaler\")\n",
    "ax[0].set_xlim(min(scaled_time), max(scaled_time))\n",
    "\n",
    "sns.distplot(loged_time, ax=ax[1], color='b', axlabel=\"LogtScaler\")\n",
    "ax[1].set_xlim(min(loged_time), max(loged_time))\n",
    "\n",
    "sns.distplot(std_time, ax=ax[2], color='b', axlabel=\"StandardScaler\")\n",
    "ax[2].set_xlim(min(std_time), max(std_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bea276",
   "metadata": {},
   "source": [
    "### * **RobustScaler를 활용한 Time 데이터 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7eaba19",
   "metadata": {
    "executionInfo": {
     "elapsed": 5178,
     "status": "ok",
     "timestamp": 1620908255876,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "f7eaba19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99498349],\n",
       "       [-0.99498349],\n",
       "       [-0.99497175],\n",
       "       ...,\n",
       "       [ 1.03497457],\n",
       "       [ 1.03497457],\n",
       "       [ 1.03502156]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "time_1 = dataset[:,0]\n",
    "time_1 = time_1.reshape(-1, 1)\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "time_data_robust = rob_scaler.fit(time_1).transform(time_1)\n",
    "time_data_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b64863f",
   "metadata": {
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "0b64863f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.034951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.034963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.034975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>1.034975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.035022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalTime\n",
       "0        -0.994983\n",
       "1        -0.994983\n",
       "2        -0.994972\n",
       "3        -0.994972\n",
       "4        -0.994960\n",
       "...            ...\n",
       "284802    1.034951\n",
       "284803    1.034963\n",
       "284804    1.034975\n",
       "284805    1.034975\n",
       "284806    1.035022\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_data = pd.DataFrame(amount_data_standard)\n",
    "time_data = pd.DataFrame(time_data_robust)\n",
    "time_data.columns = ['normalTime']\n",
    "time_data\n",
    "# print(type(time_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28060763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4393,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "28060763",
    "outputId": "b6dff9b9-03ba-41a5-a5ea-c6f850d6e266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalAmount\n",
       "0           0.244964\n",
       "1          -0.342475\n",
       "2           1.160686\n",
       "3           0.140534\n",
       "4          -0.073403\n",
       "...              ...\n",
       "284802     -0.350151\n",
       "284803     -0.254117\n",
       "284804     -0.081839\n",
       "284805     -0.313249\n",
       "284806      0.514355\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 된 Amount data 컬럼명 normalAmount로 지정\n",
    "amount_data.columns = ['normalAmount']\n",
    "amount_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f03ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 3740,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "51f03ef5",
    "outputId": "169441e6-289d-4356-ea2d-92dbf11f81fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-1.996499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-1.996394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10  ...       V23       V24       V25       V26  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.110474  0.066928  0.128539 -0.189115   \n",
       "1  0.085102 -0.255425 -0.166974  ...  0.101288 -0.339846  0.167170  0.125895   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.137458  0.141267 -0.206010  0.502292   \n",
       "5  0.260314 -0.568671 -0.371407  ... -0.026398 -0.371427 -0.232794  0.105915   \n",
       "6  0.081213  0.464960 -0.099254  ... -0.154104 -0.780055  0.750137 -0.257237   \n",
       "7 -3.807864  0.615375  1.249376  ...  0.057504 -0.649709 -0.415267 -0.051634   \n",
       "8  0.851084 -0.392048 -0.410430  ... -0.204233  1.011592  0.373205 -0.384157   \n",
       "9  0.069539 -0.736727 -0.366846  ... -0.120794 -0.385050 -0.069733  0.094199   \n",
       "\n",
       "        V27       V28  Class  scaled_time  loged_time  std_time  \n",
       "0  0.133558 -0.021053      0    -0.994983    0.000000 -1.996583  \n",
       "1 -0.008983  0.014724      0    -0.994983    0.000000 -1.996583  \n",
       "2 -0.055353 -0.059752      0    -0.994972    0.693147 -1.996562  \n",
       "3  0.062723  0.061458      0    -0.994972    0.693147 -1.996562  \n",
       "4  0.219422  0.215153      0    -0.994960    1.098612 -1.996541  \n",
       "5  0.253844  0.081080      0    -0.994960    1.098612 -1.996541  \n",
       "6  0.034507  0.005168      0    -0.994937    1.609438 -1.996499  \n",
       "7 -1.206921 -1.085339      0    -0.994901    2.079442 -1.996436  \n",
       "8  0.011747  0.142404      0    -0.994901    2.079442 -1.996436  \n",
       "9  0.246219  0.083076      0    -0.994878    2.302585 -1.996394  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존에 있던 Time, Amount 데이터 drop\n",
    "df.drop('Time', axis=1, inplace = True)\n",
    "df.drop('Amount', axis=1, inplace = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6c9fad0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 1686,
     "status": "ok",
     "timestamp": 1620908256261,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "e6c9fad0",
    "outputId": "48ac4679-711f-4a82-d262-b49fa265d228",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.338556</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.333279</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-1.996499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.190107</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019392</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338516</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-1.996394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   normalTime        V1        V2        V3        V4        V5        V6  \\\n",
       "0    0.244964 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1   -0.342475  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    1.160686 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3    0.140534 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4   -0.073403 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5   -0.338556 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6   -0.333279  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7   -0.190107 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8    0.019392 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9   -0.338516 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "\n",
       "         V7        V8        V9  ...       V24       V25       V26       V27  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.066928  0.128539 -0.189115  0.133558   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.339846  0.167170  0.125895 -0.008983   \n",
       "2  0.791461  0.247676 -1.514654  ... -0.689281 -0.327642 -0.139097 -0.055353   \n",
       "3  0.237609  0.377436 -1.387024  ... -1.175575  0.647376 -0.221929  0.062723   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.141267 -0.206010  0.502292  0.219422   \n",
       "5  0.476201  0.260314 -0.568671  ... -0.371427 -0.232794  0.105915  0.253844   \n",
       "6 -0.005159  0.081213  0.464960  ... -0.780055  0.750137 -0.257237  0.034507   \n",
       "7  1.120631 -3.807864  0.615375  ... -0.649709 -0.415267 -0.051634 -1.206921   \n",
       "8  0.370145  0.851084 -0.392048  ...  1.011592  0.373205 -0.384157  0.011747   \n",
       "9  0.651583  0.069539 -0.736727  ... -0.385050 -0.069733  0.094199  0.246219   \n",
       "\n",
       "        V28  normalAmount  Class  scaled_time  loged_time  std_time  \n",
       "0 -0.021053     -0.994983      0    -0.994983    0.000000 -1.996583  \n",
       "1  0.014724     -0.994983      0    -0.994983    0.000000 -1.996583  \n",
       "2 -0.059752     -0.994972      0    -0.994972    0.693147 -1.996562  \n",
       "3  0.061458     -0.994972      0    -0.994972    0.693147 -1.996562  \n",
       "4  0.215153     -0.994960      0    -0.994960    1.098612 -1.996541  \n",
       "5  0.081080     -0.994960      0    -0.994960    1.098612 -1.996541  \n",
       "6  0.005168     -0.994937      0    -0.994937    1.609438 -1.996499  \n",
       "7 -1.085339     -0.994901      0    -0.994901    2.079442 -1.996436  \n",
       "8  0.142404     -0.994901      0    -0.994901    2.079442 -1.996436  \n",
       "9  0.083076     -0.994878      0    -0.994878    2.302585 -1.996394  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화된 Time,Amount data 삽입\n",
    "df.insert(0,'normalTime', amount_data )\n",
    "df.insert(29,'normalAmount',time_data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c198d",
   "metadata": {},
   "source": [
    "### * **Class = 1, Class = 0값 비율을 위한 샘플링 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c53a8202",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106839</th>\n",
       "      <td>-0.265311</td>\n",
       "      <td>1.411190</td>\n",
       "      <td>-0.713597</td>\n",
       "      <td>0.386150</td>\n",
       "      <td>-1.007847</td>\n",
       "      <td>-0.577021</td>\n",
       "      <td>0.565564</td>\n",
       "      <td>-1.062723</td>\n",
       "      <td>0.113954</td>\n",
       "      <td>-0.433717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.248912</td>\n",
       "      <td>0.565982</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>0.055746</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>-0.170820</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.170820</td>\n",
       "      <td>11.158434</td>\n",
       "      <td>-0.519328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72757</th>\n",
       "      <td>-0.346073</td>\n",
       "      <td>-2.986466</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>0.605887</td>\n",
       "      <td>0.338338</td>\n",
       "      <td>0.685448</td>\n",
       "      <td>-1.581954</td>\n",
       "      <td>0.504206</td>\n",
       "      <td>-0.233403</td>\n",
       "      <td>0.636768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355065</td>\n",
       "      <td>0.448552</td>\n",
       "      <td>0.193490</td>\n",
       "      <td>1.214588</td>\n",
       "      <td>-0.013923</td>\n",
       "      <td>-0.350639</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.350639</td>\n",
       "      <td>10.912303</td>\n",
       "      <td>-0.841640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155713</th>\n",
       "      <td>-0.308171</td>\n",
       "      <td>-0.513840</td>\n",
       "      <td>0.752265</td>\n",
       "      <td>2.375663</td>\n",
       "      <td>-0.104836</td>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.274978</td>\n",
       "      <td>0.502985</td>\n",
       "      <td>-0.213632</td>\n",
       "      <td>1.615484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136685</td>\n",
       "      <td>0.229910</td>\n",
       "      <td>-0.722749</td>\n",
       "      <td>-0.084214</td>\n",
       "      <td>-0.147139</td>\n",
       "      <td>0.252599</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252599</td>\n",
       "      <td>11.573023</td>\n",
       "      <td>0.239621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>-0.307691</td>\n",
       "      <td>-4.641893</td>\n",
       "      <td>2.902086</td>\n",
       "      <td>-1.572939</td>\n",
       "      <td>2.507299</td>\n",
       "      <td>-0.871783</td>\n",
       "      <td>-1.040903</td>\n",
       "      <td>-1.593901</td>\n",
       "      <td>-3.254905</td>\n",
       "      <td>1.908963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645545</td>\n",
       "      <td>-0.354558</td>\n",
       "      <td>-0.611764</td>\n",
       "      <td>-3.908080</td>\n",
       "      <td>-0.671248</td>\n",
       "      <td>-0.809161</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.809161</td>\n",
       "      <td>9.668904</td>\n",
       "      <td>-1.663510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231978</th>\n",
       "      <td>-0.321245</td>\n",
       "      <td>-2.064240</td>\n",
       "      <td>2.629739</td>\n",
       "      <td>-0.748406</td>\n",
       "      <td>0.694992</td>\n",
       "      <td>0.418178</td>\n",
       "      <td>1.392520</td>\n",
       "      <td>-1.697801</td>\n",
       "      <td>-6.333065</td>\n",
       "      <td>1.724184</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.051685</td>\n",
       "      <td>0.209178</td>\n",
       "      <td>-0.319859</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>-0.050117</td>\n",
       "      <td>0.731987</td>\n",
       "      <td>1</td>\n",
       "      <td>0.731987</td>\n",
       "      <td>11.898181</td>\n",
       "      <td>1.098890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>0.422399</td>\n",
       "      <td>0.861545</td>\n",
       "      <td>-1.065624</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.570590</td>\n",
       "      <td>-1.205622</td>\n",
       "      <td>0.270839</td>\n",
       "      <td>-0.673109</td>\n",
       "      <td>0.221099</td>\n",
       "      <td>-0.896328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034928</td>\n",
       "      <td>0.180498</td>\n",
       "      <td>-0.303340</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.054071</td>\n",
       "      <td>-0.586661</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.586661</td>\n",
       "      <td>10.456136</td>\n",
       "      <td>-1.264694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64436</th>\n",
       "      <td>-0.280184</td>\n",
       "      <td>1.593954</td>\n",
       "      <td>-1.041333</td>\n",
       "      <td>-1.593685</td>\n",
       "      <td>-2.524160</td>\n",
       "      <td>1.340290</td>\n",
       "      <td>2.947308</td>\n",
       "      <td>-1.047326</td>\n",
       "      <td>0.614772</td>\n",
       "      <td>-2.322846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943166</td>\n",
       "      <td>0.499233</td>\n",
       "      <td>-0.434746</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>-0.394119</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394119</td>\n",
       "      <td>10.842440</td>\n",
       "      <td>-0.919576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190368</th>\n",
       "      <td>0.350034</td>\n",
       "      <td>-2.272473</td>\n",
       "      <td>2.935226</td>\n",
       "      <td>-4.871394</td>\n",
       "      <td>2.419012</td>\n",
       "      <td>-1.513022</td>\n",
       "      <td>-0.480625</td>\n",
       "      <td>-2.126136</td>\n",
       "      <td>1.883507</td>\n",
       "      <td>-1.297262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322913</td>\n",
       "      <td>-0.058406</td>\n",
       "      <td>-0.411649</td>\n",
       "      <td>0.573803</td>\n",
       "      <td>0.176067</td>\n",
       "      <td>0.518227</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518227</td>\n",
       "      <td>11.766047</td>\n",
       "      <td>0.715741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57349</th>\n",
       "      <td>-0.277186</td>\n",
       "      <td>-0.334815</td>\n",
       "      <td>1.189703</td>\n",
       "      <td>1.748604</td>\n",
       "      <td>2.729949</td>\n",
       "      <td>0.291077</td>\n",
       "      <td>0.468765</td>\n",
       "      <td>0.478128</td>\n",
       "      <td>0.192521</td>\n",
       "      <td>-1.845558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262088</td>\n",
       "      <td>-0.326833</td>\n",
       "      <td>0.330151</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>-0.432583</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.432583</td>\n",
       "      <td>10.776286</td>\n",
       "      <td>-0.988519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200319</th>\n",
       "      <td>-0.345313</td>\n",
       "      <td>2.053859</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>-1.044302</td>\n",
       "      <td>0.398776</td>\n",
       "      <td>-0.017421</td>\n",
       "      <td>-1.103648</td>\n",
       "      <td>0.242713</td>\n",
       "      <td>-0.369792</td>\n",
       "      <td>0.378501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>-0.277432</td>\n",
       "      <td>0.197468</td>\n",
       "      <td>-0.060623</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>0.572081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572081</td>\n",
       "      <td>11.801017</td>\n",
       "      <td>0.812270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150692</th>\n",
       "      <td>-0.201582</td>\n",
       "      <td>-11.320633</td>\n",
       "      <td>7.191950</td>\n",
       "      <td>-13.179083</td>\n",
       "      <td>9.099552</td>\n",
       "      <td>-10.094749</td>\n",
       "      <td>-2.440115</td>\n",
       "      <td>-14.184337</td>\n",
       "      <td>4.452503</td>\n",
       "      <td>-6.241960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661133</td>\n",
       "      <td>-0.396522</td>\n",
       "      <td>-0.413315</td>\n",
       "      <td>-0.997548</td>\n",
       "      <td>-0.235036</td>\n",
       "      <td>0.108225</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108225</td>\n",
       "      <td>11.450039</td>\n",
       "      <td>-0.019160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258056</th>\n",
       "      <td>0.949427</td>\n",
       "      <td>-4.309441</td>\n",
       "      <td>-2.919986</td>\n",
       "      <td>1.405447</td>\n",
       "      <td>0.994070</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>1.006933</td>\n",
       "      <td>1.755872</td>\n",
       "      <td>-0.976941</td>\n",
       "      <td>1.846423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705787</td>\n",
       "      <td>0.811982</td>\n",
       "      <td>0.376536</td>\n",
       "      <td>-0.744661</td>\n",
       "      <td>1.146026</td>\n",
       "      <td>0.866763</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866763</td>\n",
       "      <td>11.973327</td>\n",
       "      <td>1.340466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15849</th>\n",
       "      <td>-0.171476</td>\n",
       "      <td>-1.242069</td>\n",
       "      <td>1.357780</td>\n",
       "      <td>1.142481</td>\n",
       "      <td>2.943035</td>\n",
       "      <td>-0.381281</td>\n",
       "      <td>0.164104</td>\n",
       "      <td>0.311346</td>\n",
       "      <td>0.138942</td>\n",
       "      <td>-0.938993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521447</td>\n",
       "      <td>-0.630640</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>-0.483905</td>\n",
       "      <td>-0.146492</td>\n",
       "      <td>-0.674350</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.674350</td>\n",
       "      <td>10.214386</td>\n",
       "      <td>-1.421870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115543</th>\n",
       "      <td>-0.350191</td>\n",
       "      <td>1.060241</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>1.019393</td>\n",
       "      <td>2.582009</td>\n",
       "      <td>-0.304338</td>\n",
       "      <td>0.454251</td>\n",
       "      <td>-0.288715</td>\n",
       "      <td>0.325218</td>\n",
       "      <td>-0.408663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165274</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>-0.176277</td>\n",
       "      <td>0.016360</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>-0.126740</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.126740</td>\n",
       "      <td>11.210536</td>\n",
       "      <td>-0.440318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68067</th>\n",
       "      <td>1.725376</td>\n",
       "      <td>-1.101847</td>\n",
       "      <td>-1.632441</td>\n",
       "      <td>0.901067</td>\n",
       "      <td>0.847753</td>\n",
       "      <td>-1.249091</td>\n",
       "      <td>0.654937</td>\n",
       "      <td>1.448868</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>-0.136742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029091</td>\n",
       "      <td>-0.300896</td>\n",
       "      <td>0.699175</td>\n",
       "      <td>-0.336072</td>\n",
       "      <td>-0.177587</td>\n",
       "      <td>-0.374511</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.374511</td>\n",
       "      <td>10.874551</td>\n",
       "      <td>-0.884430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220725</th>\n",
       "      <td>0.944509</td>\n",
       "      <td>-1.169203</td>\n",
       "      <td>1.863414</td>\n",
       "      <td>-2.515135</td>\n",
       "      <td>5.463681</td>\n",
       "      <td>-0.297971</td>\n",
       "      <td>1.364918</td>\n",
       "      <td>0.759219</td>\n",
       "      <td>-0.118861</td>\n",
       "      <td>-2.293921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078616</td>\n",
       "      <td>-0.544655</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>-0.240930</td>\n",
       "      <td>-0.781055</td>\n",
       "      <td>0.676559</td>\n",
       "      <td>1</td>\n",
       "      <td>0.676559</td>\n",
       "      <td>11.865559</td>\n",
       "      <td>0.999538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150684</th>\n",
       "      <td>-0.349231</td>\n",
       "      <td>-10.040631</td>\n",
       "      <td>6.139183</td>\n",
       "      <td>-12.972972</td>\n",
       "      <td>7.740555</td>\n",
       "      <td>-8.684705</td>\n",
       "      <td>-3.837429</td>\n",
       "      <td>-11.907702</td>\n",
       "      <td>5.833273</td>\n",
       "      <td>-5.731054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843012</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>-0.307375</td>\n",
       "      <td>0.061631</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>11.449869</td>\n",
       "      <td>-0.019497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212354</th>\n",
       "      <td>-0.350231</td>\n",
       "      <td>-0.969974</td>\n",
       "      <td>1.515210</td>\n",
       "      <td>-0.620161</td>\n",
       "      <td>-0.497631</td>\n",
       "      <td>0.483776</td>\n",
       "      <td>-1.303317</td>\n",
       "      <td>0.875425</td>\n",
       "      <td>-0.110432</td>\n",
       "      <td>0.366743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>-0.619142</td>\n",
       "      <td>-0.235078</td>\n",
       "      <td>0.547562</td>\n",
       "      <td>0.242167</td>\n",
       "      <td>0.635933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.635933</td>\n",
       "      <td>11.840955</td>\n",
       "      <td>0.926720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266783</th>\n",
       "      <td>-0.342914</td>\n",
       "      <td>0.140872</td>\n",
       "      <td>0.926565</td>\n",
       "      <td>-0.621609</td>\n",
       "      <td>-0.778584</td>\n",
       "      <td>1.179388</td>\n",
       "      <td>-0.221437</td>\n",
       "      <td>0.815257</td>\n",
       "      <td>0.106577</td>\n",
       "      <td>-0.174969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>-0.400130</td>\n",
       "      <td>0.130450</td>\n",
       "      <td>0.213862</td>\n",
       "      <td>0.067348</td>\n",
       "      <td>0.913897</td>\n",
       "      <td>0</td>\n",
       "      <td>0.913897</td>\n",
       "      <td>11.998329</td>\n",
       "      <td>1.424951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6971</th>\n",
       "      <td>6.882027</td>\n",
       "      <td>-3.499108</td>\n",
       "      <td>0.258555</td>\n",
       "      <td>-4.489558</td>\n",
       "      <td>4.853894</td>\n",
       "      <td>-6.974522</td>\n",
       "      <td>3.628382</td>\n",
       "      <td>5.431271</td>\n",
       "      <td>-1.946734</td>\n",
       "      <td>-0.775680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170279</td>\n",
       "      <td>-0.393844</td>\n",
       "      <td>0.296367</td>\n",
       "      <td>1.985913</td>\n",
       "      <td>-0.900452</td>\n",
       "      <td>-0.888497</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.888497</td>\n",
       "      <td>9.112176</td>\n",
       "      <td>-1.805714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalTime         V1        V2         V3        V4         V5  \\\n",
       "106839   -0.265311   1.411190 -0.713597   0.386150 -1.007847  -0.577021   \n",
       "72757    -0.346073  -2.986466 -0.000891   0.605887  0.338338   0.685448   \n",
       "155713   -0.308171  -0.513840  0.752265   2.375663 -0.104836   0.060729   \n",
       "10204    -0.307691  -4.641893  2.902086  -1.572939  2.507299  -0.871783   \n",
       "231978   -0.321245  -2.064240  2.629739  -0.748406  0.694992   0.418178   \n",
       "27895     0.422399   0.861545 -1.065624   0.672919  0.570590  -1.205622   \n",
       "64436    -0.280184   1.593954 -1.041333  -1.593685 -2.524160   1.340290   \n",
       "190368    0.350034  -2.272473  2.935226  -4.871394  2.419012  -1.513022   \n",
       "57349    -0.277186  -0.334815  1.189703   1.748604  2.729949   0.291077   \n",
       "200319   -0.345313   2.053859  0.017620  -1.044302  0.398776  -0.017421   \n",
       "150692   -0.201582 -11.320633  7.191950 -13.179083  9.099552 -10.094749   \n",
       "258056    0.949427  -4.309441 -2.919986   1.405447  0.994070  -0.004181   \n",
       "15849    -0.171476  -1.242069  1.357780   1.142481  2.943035  -0.381281   \n",
       "115543   -0.350191   1.060241  0.294210   1.019393  2.582009  -0.304338   \n",
       "68067     1.725376  -1.101847 -1.632441   0.901067  0.847753  -1.249091   \n",
       "220725    0.944509  -1.169203  1.863414  -2.515135  5.463681  -0.297971   \n",
       "150684   -0.349231 -10.040631  6.139183 -12.972972  7.740555  -8.684705   \n",
       "212354   -0.350231  -0.969974  1.515210  -0.620161 -0.497631   0.483776   \n",
       "266783   -0.342914   0.140872  0.926565  -0.621609 -0.778584   1.179388   \n",
       "6971      6.882027  -3.499108  0.258555  -4.489558  4.853894  -6.974522   \n",
       "\n",
       "              V6         V7        V8        V9  ...       V24       V25  \\\n",
       "106839  0.565564  -1.062723  0.113954 -0.433717  ... -1.248912  0.565982   \n",
       "72757  -1.581954   0.504206 -0.233403  0.636768  ...  0.355065  0.448552   \n",
       "155713 -0.274978   0.502985 -0.213632  1.615484  ... -0.136685  0.229910   \n",
       "10204  -1.040903  -1.593901 -3.254905  1.908963  ...  0.645545 -0.354558   \n",
       "231978  1.392520  -1.697801 -6.333065  1.724184  ... -1.051685  0.209178   \n",
       "27895   0.270839  -0.673109  0.221099 -0.896328  ... -0.034928  0.180498   \n",
       "64436   2.947308  -1.047326  0.614772 -2.322846  ...  0.943166  0.499233   \n",
       "190368 -0.480625  -2.126136  1.883507 -1.297262  ...  0.322913 -0.058406   \n",
       "57349   0.468765   0.478128  0.192521 -1.845558  ...  0.262088 -0.326833   \n",
       "200319 -1.103648   0.242713 -0.369792  0.378501  ...  0.015987 -0.277432   \n",
       "150692 -2.440115 -14.184337  4.452503 -6.241960  ...  0.661133 -0.396522   \n",
       "258056  1.006933   1.755872 -0.976941  1.846423  ...  0.705787  0.811982   \n",
       "15849   0.164104   0.311346  0.138942 -0.938993  ...  0.521447 -0.630640   \n",
       "115543  0.454251  -0.288715  0.325218 -0.408663  ...  0.165274  0.213565   \n",
       "68067   0.654937   1.448868  0.023308 -0.136742  ... -0.029091 -0.300896   \n",
       "220725  1.364918   0.759219 -0.118861 -2.293921  ... -0.078616 -0.544655   \n",
       "150684 -3.837429 -11.907702  5.833273 -5.731054  ...  0.843012  0.549938   \n",
       "212354 -1.303317   0.875425 -0.110432  0.366743  ...  0.053954 -0.619142   \n",
       "266783 -0.221437   0.815257  0.106577 -0.174969  ... -0.029549 -0.400130   \n",
       "6971    3.628382   5.431271 -1.946734 -0.775680  ...  0.170279 -0.393844   \n",
       "\n",
       "             V26       V27       V28  normalAmount  Class  scaled_time  \\\n",
       "106839  0.062935  0.055746  0.011886     -0.170820      0    -0.170820   \n",
       "72757   0.193490  1.214588 -0.013923     -0.350639      1    -0.350639   \n",
       "155713 -0.722749 -0.084214 -0.147139      0.252599      0     0.252599   \n",
       "10204  -0.611764 -3.908080 -0.671248     -0.809161      1    -0.809161   \n",
       "231978 -0.319859  0.015434 -0.050117      0.731987      1     0.731987   \n",
       "27895  -0.303340  0.039835  0.054071     -0.586661      0    -0.586661   \n",
       "64436  -0.434746  0.004902  0.009553     -0.394119      0    -0.394119   \n",
       "190368 -0.411649  0.573803  0.176067      0.518227      1     0.518227   \n",
       "57349   0.330151  0.151411  0.135550     -0.432583      0    -0.432583   \n",
       "200319  0.197468 -0.060623 -0.058174      0.572081      0     0.572081   \n",
       "150692 -0.413315 -0.997548 -0.235036      0.108225      1     0.108225   \n",
       "258056  0.376536 -0.744661  1.146026      0.866763      0     0.866763   \n",
       "15849  -0.093659 -0.483905 -0.146492     -0.674350      0    -0.674350   \n",
       "115543 -0.176277  0.016360  0.012227     -0.126740      0    -0.126740   \n",
       "68067   0.699175 -0.336072 -0.177587     -0.374511      1    -0.374511   \n",
       "220725  0.014777 -0.240930 -0.781055      0.676559      1     0.676559   \n",
       "150684  0.113892 -0.307375  0.061631      0.108037      1     0.108037   \n",
       "212354 -0.235078  0.547562  0.242167      0.635933      0     0.635933   \n",
       "266783  0.130450  0.213862  0.067348      0.913897      0     0.913897   \n",
       "6971    0.296367  1.985913 -0.900452     -0.888497      1    -0.888497   \n",
       "\n",
       "        loged_time  std_time  \n",
       "106839   11.158434 -0.519328  \n",
       "72757    10.912303 -0.841640  \n",
       "155713   11.573023  0.239621  \n",
       "10204     9.668904 -1.663510  \n",
       "231978   11.898181  1.098890  \n",
       "27895    10.456136 -1.264694  \n",
       "64436    10.842440 -0.919576  \n",
       "190368   11.766047  0.715741  \n",
       "57349    10.776286 -0.988519  \n",
       "200319   11.801017  0.812270  \n",
       "150692   11.450039 -0.019160  \n",
       "258056   11.973327  1.340466  \n",
       "15849    10.214386 -1.421870  \n",
       "115543   11.210536 -0.440318  \n",
       "68067    10.874551 -0.884430  \n",
       "220725   11.865559  0.999538  \n",
       "150684   11.449869 -0.019497  \n",
       "212354   11.840955  0.926720  \n",
       "266783   11.998329  1.424951  \n",
       "6971      9.112176 -1.805714  \n",
       "\n",
       "[20 rows x 34 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df  = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df= pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab843362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    492\n",
       "1    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44a77a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48816552",
   "metadata": {
    "id": "48816552"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ead22",
   "metadata": {
    "id": "2e6ead22"
   },
   "source": [
    "## 3. Class를 종속변수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb699ae0",
   "metadata": {
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1620908261886,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb699ae0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "X = df.values[:,0:30]\n",
    "Y = df.values[:,30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ed960",
   "metadata": {},
   "source": [
    "## 4. Train Set : Test set = 85:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70420ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6792f",
   "metadata": {},
   "source": [
    "## 5. ANN/DNN활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b005696",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 258196,
     "status": "error",
     "timestamp": 1620908671189,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "3b005696",
    "outputId": "50445b54-393e-45c0-b516-26a04c5d3fe5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.2759 - accuracy: 0.5115 - val_loss: 0.9241 - val_accuracy: 0.4841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92415, saving model to ./model\\01-0.9241,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\01-0.9241,hdf5\\assets\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8384 - accuracy: 0.5548 - val_loss: 0.6775 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92415 to 0.67755, saving model to ./model\\02-0.6775,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\02-0.6775,hdf5\\assets\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.7307 - val_loss: 0.5560 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67755 to 0.55602, saving model to ./model\\03-0.5560,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\03-0.5560,hdf5\\assets\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5309 - accuracy: 0.8552 - val_loss: 0.4937 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55602 to 0.49369, saving model to ./model\\04-0.4937,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\04-0.4937,hdf5\\assets\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.8800 - val_loss: 0.4489 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49369 to 0.44889, saving model to ./model\\05-0.4489,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\05-0.4489,hdf5\\assets\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.9127 - val_loss: 0.4097 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44889 to 0.40966, saving model to ./model\\06-0.4097,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\06-0.4097,hdf5\\assets\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3840 - accuracy: 0.9186 - val_loss: 0.3767 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40966 to 0.37668, saving model to ./model\\07-0.3767,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\07-0.3767,hdf5\\assets\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.9276 - val_loss: 0.3474 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37668 to 0.34736, saving model to ./model\\08-0.3474,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\08-0.3474,hdf5\\assets\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.9098 - val_loss: 0.3214 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34736 to 0.32138, saving model to ./model\\09-0.3214,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\09-0.3214,hdf5\\assets\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.9236 - val_loss: 0.2982 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32138 to 0.29821, saving model to ./model\\10-0.2982,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\10-0.2982,hdf5\\assets\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2575 - accuracy: 0.9306 - val_loss: 0.2797 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.29821 to 0.27975, saving model to ./model\\11-0.2797,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\11-0.2797,hdf5\\assets\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - accuracy: 0.9373 - val_loss: 0.2642 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.27975 to 0.26415, saving model to ./model\\12-0.2642,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\12-0.2642,hdf5\\assets\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9470 - val_loss: 0.2517 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.26415 to 0.25170, saving model to ./model\\13-0.2517,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\13-0.2517,hdf5\\assets\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2180 - accuracy: 0.9289 - val_loss: 0.2407 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.25170 to 0.24069, saving model to ./model\\14-0.2407,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\14-0.2407,hdf5\\assets\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9234 - val_loss: 0.2313 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.24069 to 0.23133, saving model to ./model\\15-0.2313,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\15-0.2313,hdf5\\assets\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1958 - accuracy: 0.9300 - val_loss: 0.2235 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.23133 to 0.22349, saving model to ./model\\16-0.2235,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\16-0.2235,hdf5\\assets\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9486 - val_loss: 0.2181 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22349 to 0.21806, saving model to ./model\\17-0.2181,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\17-0.2181,hdf5\\assets\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9456 - val_loss: 0.2119 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21806 to 0.21189, saving model to ./model\\18-0.2119,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\18-0.2119,hdf5\\assets\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9449 - val_loss: 0.2056 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.21189 to 0.20562, saving model to ./model\\19-0.2056,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\19-0.2056,hdf5\\assets\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1653 - accuracy: 0.9440 - val_loss: 0.2018 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.20562 to 0.20176, saving model to ./model\\20-0.2018,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\20-0.2018,hdf5\\assets\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9542 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.20176 to 0.20079, saving model to ./model\\21-0.2008,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\21-0.2008,hdf5\\assets\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9583 - val_loss: 0.1986 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.20079 to 0.19861, saving model to ./model\\22-0.1986,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\22-0.1986,hdf5\\assets\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9552 - val_loss: 0.1970 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.19861 to 0.19698, saving model to ./model\\23-0.1970,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\23-0.1970,hdf5\\assets\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9643 - val_loss: 0.1949 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.19698 to 0.19489, saving model to ./model\\24-0.1949,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\24-0.1949,hdf5\\assets\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9515 - val_loss: 0.1904 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19489 to 0.19040, saving model to ./model\\25-0.1904,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\25-0.1904,hdf5\\assets\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9598 - val_loss: 0.1880 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.19040 to 0.18796, saving model to ./model\\26-0.1880,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\26-0.1880,hdf5\\assets\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9590 - val_loss: 0.1860 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.18796 to 0.18597, saving model to ./model\\27-0.1860,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\27-0.1860,hdf5\\assets\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9544 - val_loss: 0.1843 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.18597 to 0.18434, saving model to ./model\\28-0.1843,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\28-0.1843,hdf5\\assets\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9539 - val_loss: 0.1794 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.18434 to 0.17944, saving model to ./model\\29-0.1794,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\29-0.1794,hdf5\\assets\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 0.1782 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.17944 to 0.17821, saving model to ./model\\30-0.1782,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\30-0.1782,hdf5\\assets\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.9596 - val_loss: 0.1797 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.17821\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9590 - val_loss: 0.1799 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.17821\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9607 - val_loss: 0.1786 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.17821\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1075 - accuracy: 0.9581 - val_loss: 0.1759 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.17821 to 0.17585, saving model to ./model\\34-0.1759,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\34-0.1759,hdf5\\assets\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9655 - val_loss: 0.1754 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.17585 to 0.17536, saving model to ./model\\35-0.1754,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\35-0.1754,hdf5\\assets\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9616 - val_loss: 0.1770 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.17536\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9655 - val_loss: 0.1754 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.17536\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9689 - val_loss: 0.1715 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.17536 to 0.17147, saving model to ./model\\38-0.1715,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\38-0.1715,hdf5\\assets\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1086 - accuracy: 0.9613 - val_loss: 0.1704 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.17147 to 0.17039, saving model to ./model\\39-0.1704,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\39-0.1704,hdf5\\assets\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9658 - val_loss: 0.1728 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.17039\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9648 - val_loss: 0.1746 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.17039\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9656 - val_loss: 0.1759 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.17039\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9685 - val_loss: 0.1750 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.17039\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9625 - val_loss: 0.1719 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.17039\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9708 - val_loss: 0.1710 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.17039\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9706 - val_loss: 0.1698 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.17039 to 0.16981, saving model to ./model\\46-0.1698,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\46-0.1698,hdf5\\assets\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9736 - val_loss: 0.1697 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.16981 to 0.16970, saving model to ./model\\47-0.1697,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\47-0.1697,hdf5\\assets\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 0.1721 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.16970\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9689 - val_loss: 0.1750 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.16970\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9701 - val_loss: 0.1779 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.16970\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9751 - val_loss: 0.1814 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.16970\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9735 - val_loss: 0.1841 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.16970\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9677 - val_loss: 0.1818 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.16970\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.9738 - val_loss: 0.1830 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.16970\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9646 - val_loss: 0.1846 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.16970\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9734 - val_loss: 0.1871 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.16970\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9774 - val_loss: 0.1894 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.16970\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0756 - accuracy: 0.9756 - val_loss: 0.1906 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.16970\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9796 - val_loss: 0.1916 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.16970\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.9802 - val_loss: 0.1927 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.16970\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.9776 - val_loss: 0.1949 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.16970\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9730 - val_loss: 0.1951 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.16970\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9795 - val_loss: 0.1968 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.16970\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9801 - val_loss: 0.1975 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.16970\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9781 - val_loss: 0.1947 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_loss did not improve from 0.16970\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9759 - val_loss: 0.1990 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.16970\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9876 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.16970\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9756 - val_loss: 0.2017 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.16970\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9850 - val_loss: 0.2028 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.16970\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9852 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.16970\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.2020 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.16970\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9817 - val_loss: 0.2057 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.16970\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9855 - val_loss: 0.2118 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.16970\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9868 - val_loss: 0.2140 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.16970\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9848 - val_loss: 0.2165 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.16970\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9853 - val_loss: 0.2166 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.16970\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9853 - val_loss: 0.2190 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.16970\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9881 - val_loss: 0.2203 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.16970\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.2219 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.16970\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9863 - val_loss: 0.2228 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.16970\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 0.2236 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.16970\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.2251 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.16970\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9829 - val_loss: 0.2276 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.16970\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9915 - val_loss: 0.2271 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.16970\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9882 - val_loss: 0.2280 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.16970\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0445 - accuracy: 0.9921 - val_loss: 0.2339 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.16970\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0467 - accuracy: 0.9917 - val_loss: 0.2328 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.16970\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9863 - val_loss: 0.2328 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.16970\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.2333 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.16970\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.2351 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.16970\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9918 - val_loss: 0.2352 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.16970\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9894 - val_loss: 0.2382 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.16970\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9913 - val_loss: 0.2459 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.16970\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9865 - val_loss: 0.2483 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.16970\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9899 - val_loss: 0.2507 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.16970\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9928 - val_loss: 0.2510 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.16970\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9906 - val_loss: 0.2508 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.16970\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.2509 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.16970\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9927 - val_loss: 0.2541 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.16970\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9925 - val_loss: 0.2500 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.16970\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9916 - val_loss: 0.2516 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.16970\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9905 - val_loss: 0.2570 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.16970\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9948 - val_loss: 0.2612 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.16970\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9904 - val_loss: 0.2620 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.16970\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0313 - accuracy: 0.9929 - val_loss: 0.2636 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.16970\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.2615 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.16970\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.2614 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.16970\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9911 - val_loss: 0.2679 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.16970\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9912 - val_loss: 0.2725 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.16970\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9916 - val_loss: 0.2733 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.16970\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9911 - val_loss: 0.2748 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.16970\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9933 - val_loss: 0.2690 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.16970\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9901 - val_loss: 0.2713 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.16970\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9933 - val_loss: 0.2763 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.16970\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9916 - val_loss: 0.2792 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.16970\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.2891 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.16970\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.9914 - val_loss: 0.2919 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.16970\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.2920 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.16970\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.2920 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.16970\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9953 - val_loss: 0.2938 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.16970\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.9969 - val_loss: 0.3019 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.16970\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.3038 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.16970\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 0.3039 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.16970\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9934 - val_loss: 0.3056 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.16970\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9943 - val_loss: 0.3077 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.16970\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9951 - val_loss: 0.3096 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.16970\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9939 - val_loss: 0.3123 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.16970\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9961 - val_loss: 0.3140 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.16970\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.3159 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.16970\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.3196 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.16970\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.9955 - val_loss: 0.3175 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.16970\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.9921 - val_loss: 0.3209 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.16970\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.9964 - val_loss: 0.3277 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.16970\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.3357 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.16970\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.3382 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.16970\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.3357 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.16970\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 0.3356 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.16970\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9966 - val_loss: 0.3371 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.16970\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.9964 - val_loss: 0.3403 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.16970\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 0.9971 - val_loss: 0.3415 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.16970\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9970 - val_loss: 0.3432 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.16970\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.3470 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.16970\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.3464 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.16970\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9975 - val_loss: 0.3479 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.16970\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.3509 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.16970\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.3526 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.16970\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.3560 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.16970\n",
      "테스트 손실값 : 0.14352816343307495, 테스트 정확도 : 0.9662162065505981\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "model_path='./model/{epoch:02d}-{val_loss:.4f},hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 조기 종료\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit(X_train, Y_train, validation_split=0.15,epochs = 200, batch_size = 100, callbacks=[early_stopping_callback,checkpointer])\n",
    "score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "\n",
    "print(f'테스트 손실값 : {score[0]}, 테스트 정확도 : {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03daa4",
   "metadata": {},
   "source": [
    "\n",
    "# epochs = 1500, batch_size = 5\n",
    "\n",
    "Epoch 00105: val_loss did not improve from 0.17426\n",
    "Epoch 106/1500\n",
    "142/142 [==============================] - 0s 840us/step - loss: 1.4069e-06 - accuracy: 1.0000 - val_loss: 0.8913 - val_accuracy: 0.9206\n",
    "\n",
    "Epoch 00106: val_loss did not improve from 0.17426\n",
    "Epoch 107/1500\n",
    "142/142 [==============================] - 0s 863us/step - loss: 1.8665e-06 - accuracy: 1.0000 - val_loss: 0.8933 - val_accuracy: 0.9206\n",
    "\n",
    "Epoch 00107: val_loss did not improve from 0.17426\n",
    "Epoch 108/1500\n",
    "142/142 [==============================] - 0s 821us/step - loss: 1.4894e-06 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 0.9206\n",
    "\n",
    "Epoch 00108: val_loss did not improve from 0.17426\n",
    "\n",
    "<span style=\"color:red\"> 테스트 손실값 : 0.3308659493923187, 테스트 정확도 : 0.9594594836235046 </span>\n",
    "\n",
    "---\n",
    "\n",
    "# epochs = 150, batch_size = 5\n",
    "\n",
    "Epoch 00108: val_loss did not improve from 0.13124\n",
    "Epoch 109/150\n",
    "142/142 [==============================] - 0s 711us/step - loss: 4.2829e-05 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 0.9286\n",
    "\n",
    "Epoch 00109: val_loss did not improve from 0.13124\n",
    "Epoch 110/150\n",
    "142/142 [==============================] - 0s 703us/step - loss: 4.2771e-05 - accuracy: 1.0000 - val_loss: 0.6588 - val_accuracy: 0.9286\n",
    "\n",
    "Epoch 00110: val_loss did not improve from 0.13124\n",
    "Epoch 111/150\n",
    "142/142 [==============================] - 0s 741us/step - loss: 3.1245e-05 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 0.9286\n",
    "\n",
    "Epoch 00111: val_loss did not improve from 0.13124\n",
    "\n",
    "<span style=\"color:red\"> 테스트 손실값 : 0.4469115734100342, 테스트 정확도 : 0.9459459185600281</span>\n",
    "\n",
    "---\n",
    "# epochs = 150, batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "Epoch 124/150\n",
    "15/15 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9444\n",
    "\n",
    "Epoch 00124: val_loss did not improve from 0.16326\n",
    "Epoch 125/150\n",
    "15/15 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9444\n",
    "\n",
    "Epoch 00125: val_loss did not improve from 0.16326\n",
    "\n",
    "<span style=\"color:red\"> 테스트 손실값 : 0.23782743513584137, 테스트 정확도 : 0.9594594836235046</span>\n",
    "\n",
    "---\n",
    "# epochs = 300, batch_size = 50\n",
    "\n",
    "\n",
    "Epoch 00122: val_loss did not improve from 0.16326\n",
    "Epoch 123/300\n",
    "15/15 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9444\n",
    "\n",
    "Epoch 00123: val_loss did not improve from 0.16326\n",
    "Epoch 124/300\n",
    "15/15 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9444\n",
    "\n",
    "Epoch 00124: val_loss did not improve from 0.16326\n",
    "Epoch 125/300\n",
    "15/15 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9444\n",
    "\n",
    "Epoch 00125: val_loss did not improve from 0.16326\n",
    "\n",
    "<span style=\"color:red\"> 테스트 손실값 : 0.23782743513584137, 테스트 정확도 : 0.9594594836235046</span>\n",
    "\n",
    "---\n",
    "\n",
    "# epochs = 200 batch_size = 150\n",
    "\n",
    "Epoch 00132: val_loss did not improve from 0.14258\n",
    "Epoch 133/200\n",
    "5/5 [==============================] - 0s 6ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2127 - val_accuracy: 0.9524\n",
    "\n",
    "Epoch 00133: val_loss did not improve from 0.14258\n",
    "Epoch 134/200\n",
    "5/5 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.9524\n",
    "\n",
    "Epoch 00134: val_loss did not improve from 0.14258\n",
    "Epoch 135/200\n",
    "5/5 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2172 - val_accuracy: 0.9524\n",
    "\n",
    "Epoch 00135: val_loss did not improve from 0.14258\n",
    "\n",
    "<span style=\"color:red\"> 테스트 손실값 : 0.724395751953125, 테스트 정확도 : 0.9189189076423645</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eda27de9",
   "metadata": {
    "id": "eda27de9",
    "outputId": "ddc1df91-57b2-410d-bb07-2719282bc855",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 18)                558       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,729\n",
      "Trainable params: 1,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "951761ad",
   "metadata": {
    "id": "951761ad",
    "outputId": "fda90a61-9164-4819-c6ff-99e812b3704f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "# 딕셔너리 형태\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5778e887",
   "metadata": {
    "id": "5778e887",
    "outputId": "bc990525-34af-48c5-ed3f-fbfa43e57480",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "y_value = history.history['val_loss']  # 오차값을 저장\n",
    "y_acc = history.history['accuracy']    # 정밀도를 저장\n",
    "\n",
    "print(len(y_value))\n",
    "print(len(y_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef827c8",
   "metadata": {},
   "source": [
    "### * **오차,정확도 그래프**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95774024",
   "metadata": {
    "id": "95774024",
    "outputId": "143b0849-9f58-49ec-9997-e99a7621da49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16f210bd070>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_value, c='red', markersize=3)\n",
    "plt.plot(x_len, y_acc, c='blue', markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07250b8e",
   "metadata": {},
   "source": [
    "### * **Class=1값 정확도 검증**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "900f504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_one = df[\"Class\"] == 1\n",
    "all_class_1 = df[class_one]\n",
    "\n",
    "all_class_1.drop('Class', axis=1, inplace = True)\n",
    "all_class_1.drop('scaled_time', axis=1, inplace = True)\n",
    "all_class_1.drop('loged_time', axis=1, inplace = True)\n",
    "all_class_1.drop('std_time', axis=1, inplace = True)\n",
    "all_class_1\n",
    "\n",
    "dt_one = pd.DataFrame(model.predict(all_class_1))\n",
    "plt.hist(dt_one, bins=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "module5_time_정규화_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
