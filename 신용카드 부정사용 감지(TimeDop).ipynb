{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bebfd0",
   "metadata": {
    "id": "97bebfd0"
   },
   "source": [
    "## **1. Dataset 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52703e1b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1620908248591,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "52703e1b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = DataFrame(pd.read_csv('creditcard.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d549f39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "5d549f39",
    "outputId": "5b7ac8df-de9e-4b5f-d0c4-84a8b21143f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# df.describe()\n",
    "# df.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fed5c7",
   "metadata": {},
   "source": [
    "### * **class 값 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb10e7fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb10e7fa",
    "outputId": "da4d39cd-e61a-4552-9894-a4eab3f9474e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dateset\n",
      "Frauds 0.17 %of the dateset\n"
     ]
    }
   ],
   "source": [
    "print(\"No Frauds\", round(df[\"Class\"].value_counts()[0]/len(df) * 100,2),\n",
    "     \"%of the dateset\")\n",
    "print(\"Frauds\", round(df[\"Class\"].value_counts()[1]/len(df) * 100,2),\n",
    "     \"%of the dateset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "81adf61f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1620908249134,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "81adf61f",
    "outputId": "e396d059-7ba0-4373-ea0d-ef8d3032e50a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#4374D9\", \"#F361A6\"]\n",
    "sns.countplot(\"Class\", data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ded9ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65676a4a",
   "metadata": {},
   "source": [
    "### * **StandardScaler Amount 값 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "59056a37",
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1620908249476,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "59056a37"
   },
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "amount = dataset[:,29]\n",
    "# 정규화를 위한 차원변경\n",
    "amount = amount.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4eadfbf2",
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1620908249854,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "4eadfbf2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# StandardScaler() 정규화\n",
    "standardScaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "amount_data_standard = standardScaler.fit(amount).transform(amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28060763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4393,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "28060763",
    "outputId": "b6dff9b9-03ba-41a5-a5ea-c6f850d6e266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalAmount\n",
       "0           0.244964\n",
       "1          -0.342475\n",
       "2           1.160686\n",
       "3           0.140534\n",
       "4          -0.073403\n",
       "...              ...\n",
       "284802     -0.350151\n",
       "284803     -0.254117\n",
       "284804     -0.081839\n",
       "284805     -0.313249\n",
       "284806      0.514355\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 된 Amount data 컬럼명 normalAmount로 지정\n",
    "amount_data = pd.DataFrame(amount_data_standard)\n",
    "amount_data.columns = ['normalAmount']\n",
    "amount_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "51f03ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 3740,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "51f03ef5",
    "outputId": "169441e6-289d-4356-ea2d-92dbf11f81fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "5  0.260314 -0.568671 -0.371407  ...  0.084968 -0.208254 -0.559825 -0.026398   \n",
       "6  0.081213  0.464960 -0.099254  ... -0.219633 -0.167716 -0.270710 -0.154104   \n",
       "7 -3.807864  0.615375  1.249376  ... -0.156742  1.943465 -1.015455  0.057504   \n",
       "8  0.851084 -0.392048 -0.410430  ...  0.052736 -0.073425 -0.268092 -0.204233   \n",
       "9  0.069539 -0.736727 -0.366846  ...  0.203711 -0.246914 -0.633753 -0.120794   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "5 -0.371427 -0.232794  0.105915  0.253844  0.081080      0  \n",
       "6 -0.780055  0.750137 -0.257237  0.034507  0.005168      0  \n",
       "7 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339      0  \n",
       "8  1.011592  0.373205 -0.384157  0.011747  0.142404      0  \n",
       "9 -0.385050 -0.069733  0.094199  0.246219  0.083076      0  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존에 있던 Time, Amount 데이터 drop\n",
    "df.drop('Time', axis=1, inplace = True)\n",
    "df.drop('Amount', axis=1, inplace = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6c9fad0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 1686,
     "status": "ok",
     "timestamp": 1620908256261,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "e6c9fad0",
    "outputId": "48ac4679-711f-4a82-d262-b49fa265d228",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>-0.338556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.333279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>-0.190107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>-0.338516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "5  0.260314 -0.568671 -0.371407  ... -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  0.081213  0.464960 -0.099254  ... -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7 -3.807864  0.615375  1.249376  ...  1.943465 -1.015455  0.057504 -0.649709   \n",
       "8  0.851084 -0.392048 -0.410430  ... -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9  0.069539 -0.736727 -0.366846  ... -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "\n",
       "        V25       V26       V27       V28  normalAmount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0.244964      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724     -0.342475      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      1.160686      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0.140534      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153     -0.073403      0  \n",
       "5 -0.232794  0.105915  0.253844  0.081080     -0.338556      0  \n",
       "6  0.750137 -0.257237  0.034507  0.005168     -0.333279      0  \n",
       "7 -0.415267 -0.051634 -1.206921 -1.085339     -0.190107      0  \n",
       "8  0.373205 -0.384157  0.011747  0.142404      0.019392      0  \n",
       "9 -0.069733  0.094199  0.246219  0.083076     -0.338516      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화된 Amount data 삽입\n",
    "df.insert(28,'normalAmount', amount_data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4c970",
   "metadata": {},
   "source": [
    "### * **Class = 1, Class = 0값 비율을 위한 샘플링 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c53a8202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106839</th>\n",
       "      <td>1.411190</td>\n",
       "      <td>-0.713597</td>\n",
       "      <td>0.386150</td>\n",
       "      <td>-1.007847</td>\n",
       "      <td>-0.577021</td>\n",
       "      <td>0.565564</td>\n",
       "      <td>-1.062723</td>\n",
       "      <td>0.113954</td>\n",
       "      <td>-0.433717</td>\n",
       "      <td>0.536901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332725</td>\n",
       "      <td>0.976893</td>\n",
       "      <td>-0.264513</td>\n",
       "      <td>-1.248912</td>\n",
       "      <td>0.565982</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>0.055746</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>-0.265311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72757</th>\n",
       "      <td>-2.986466</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>0.605887</td>\n",
       "      <td>0.338338</td>\n",
       "      <td>0.685448</td>\n",
       "      <td>-1.581954</td>\n",
       "      <td>0.504206</td>\n",
       "      <td>-0.233403</td>\n",
       "      <td>0.636768</td>\n",
       "      <td>1.010291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875146</td>\n",
       "      <td>-0.509849</td>\n",
       "      <td>1.313918</td>\n",
       "      <td>0.355065</td>\n",
       "      <td>0.448552</td>\n",
       "      <td>0.193490</td>\n",
       "      <td>1.214588</td>\n",
       "      <td>-0.013923</td>\n",
       "      <td>-0.346073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155713</th>\n",
       "      <td>-0.513840</td>\n",
       "      <td>0.752265</td>\n",
       "      <td>2.375663</td>\n",
       "      <td>-0.104836</td>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.274978</td>\n",
       "      <td>0.502985</td>\n",
       "      <td>-0.213632</td>\n",
       "      <td>1.615484</td>\n",
       "      <td>-1.132412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306357</td>\n",
       "      <td>-0.467832</td>\n",
       "      <td>-0.268527</td>\n",
       "      <td>-0.136685</td>\n",
       "      <td>0.229910</td>\n",
       "      <td>-0.722749</td>\n",
       "      <td>-0.084214</td>\n",
       "      <td>-0.147139</td>\n",
       "      <td>-0.308171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>-4.641893</td>\n",
       "      <td>2.902086</td>\n",
       "      <td>-1.572939</td>\n",
       "      <td>2.507299</td>\n",
       "      <td>-0.871783</td>\n",
       "      <td>-1.040903</td>\n",
       "      <td>-1.593901</td>\n",
       "      <td>-3.254905</td>\n",
       "      <td>1.908963</td>\n",
       "      <td>1.077418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963597</td>\n",
       "      <td>-0.217414</td>\n",
       "      <td>-0.549340</td>\n",
       "      <td>0.645545</td>\n",
       "      <td>-0.354558</td>\n",
       "      <td>-0.611764</td>\n",
       "      <td>-3.908080</td>\n",
       "      <td>-0.671248</td>\n",
       "      <td>-0.307691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231978</th>\n",
       "      <td>-2.064240</td>\n",
       "      <td>2.629739</td>\n",
       "      <td>-0.748406</td>\n",
       "      <td>0.694992</td>\n",
       "      <td>0.418178</td>\n",
       "      <td>1.392520</td>\n",
       "      <td>-1.697801</td>\n",
       "      <td>-6.333065</td>\n",
       "      <td>1.724184</td>\n",
       "      <td>-0.887242</td>\n",
       "      <td>...</td>\n",
       "      <td>6.215514</td>\n",
       "      <td>-1.276909</td>\n",
       "      <td>0.459861</td>\n",
       "      <td>-1.051685</td>\n",
       "      <td>0.209178</td>\n",
       "      <td>-0.319859</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>-0.050117</td>\n",
       "      <td>-0.321245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>0.861545</td>\n",
       "      <td>-1.065624</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.570590</td>\n",
       "      <td>-1.205622</td>\n",
       "      <td>0.270839</td>\n",
       "      <td>-0.673109</td>\n",
       "      <td>0.221099</td>\n",
       "      <td>-0.896328</td>\n",
       "      <td>0.910822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065595</td>\n",
       "      <td>-0.155664</td>\n",
       "      <td>-0.118910</td>\n",
       "      <td>-0.034928</td>\n",
       "      <td>0.180498</td>\n",
       "      <td>-0.303340</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.054071</td>\n",
       "      <td>0.422399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64436</th>\n",
       "      <td>1.593954</td>\n",
       "      <td>-1.041333</td>\n",
       "      <td>-1.593685</td>\n",
       "      <td>-2.524160</td>\n",
       "      <td>1.340290</td>\n",
       "      <td>2.947308</td>\n",
       "      <td>-1.047326</td>\n",
       "      <td>0.614772</td>\n",
       "      <td>-2.322846</td>\n",
       "      <td>1.513246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.556379</td>\n",
       "      <td>-1.421794</td>\n",
       "      <td>0.113418</td>\n",
       "      <td>0.943166</td>\n",
       "      <td>0.499233</td>\n",
       "      <td>-0.434746</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>-0.280184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190368</th>\n",
       "      <td>-2.272473</td>\n",
       "      <td>2.935226</td>\n",
       "      <td>-4.871394</td>\n",
       "      <td>2.419012</td>\n",
       "      <td>-1.513022</td>\n",
       "      <td>-0.480625</td>\n",
       "      <td>-2.126136</td>\n",
       "      <td>1.883507</td>\n",
       "      <td>-1.297262</td>\n",
       "      <td>-5.487425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718504</td>\n",
       "      <td>0.893850</td>\n",
       "      <td>-0.031632</td>\n",
       "      <td>0.322913</td>\n",
       "      <td>-0.058406</td>\n",
       "      <td>-0.411649</td>\n",
       "      <td>0.573803</td>\n",
       "      <td>0.176067</td>\n",
       "      <td>0.350034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57349</th>\n",
       "      <td>-0.334815</td>\n",
       "      <td>1.189703</td>\n",
       "      <td>1.748604</td>\n",
       "      <td>2.729949</td>\n",
       "      <td>0.291077</td>\n",
       "      <td>0.468765</td>\n",
       "      <td>0.478128</td>\n",
       "      <td>0.192521</td>\n",
       "      <td>-1.845558</td>\n",
       "      <td>0.830895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338385</td>\n",
       "      <td>0.978178</td>\n",
       "      <td>-0.151153</td>\n",
       "      <td>0.262088</td>\n",
       "      <td>-0.326833</td>\n",
       "      <td>0.330151</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>-0.277186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200319</th>\n",
       "      <td>2.053859</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>-1.044302</td>\n",
       "      <td>0.398776</td>\n",
       "      <td>-0.017421</td>\n",
       "      <td>-1.103648</td>\n",
       "      <td>0.242713</td>\n",
       "      <td>-0.369792</td>\n",
       "      <td>0.378501</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271011</td>\n",
       "      <td>-0.588222</td>\n",
       "      <td>0.333976</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>-0.277432</td>\n",
       "      <td>0.197468</td>\n",
       "      <td>-0.060623</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>-0.345313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150692</th>\n",
       "      <td>-11.320633</td>\n",
       "      <td>7.191950</td>\n",
       "      <td>-13.179083</td>\n",
       "      <td>9.099552</td>\n",
       "      <td>-10.094749</td>\n",
       "      <td>-2.440115</td>\n",
       "      <td>-14.184337</td>\n",
       "      <td>4.452503</td>\n",
       "      <td>-6.241960</td>\n",
       "      <td>-12.618163</td>\n",
       "      <td>...</td>\n",
       "      <td>1.082235</td>\n",
       "      <td>-0.350563</td>\n",
       "      <td>0.483044</td>\n",
       "      <td>0.661133</td>\n",
       "      <td>-0.396522</td>\n",
       "      <td>-0.413315</td>\n",
       "      <td>-0.997548</td>\n",
       "      <td>-0.235036</td>\n",
       "      <td>-0.201582</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258056</th>\n",
       "      <td>-4.309441</td>\n",
       "      <td>-2.919986</td>\n",
       "      <td>1.405447</td>\n",
       "      <td>0.994070</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>1.006933</td>\n",
       "      <td>1.755872</td>\n",
       "      <td>-0.976941</td>\n",
       "      <td>1.846423</td>\n",
       "      <td>0.946266</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117336</td>\n",
       "      <td>0.011099</td>\n",
       "      <td>1.111285</td>\n",
       "      <td>0.705787</td>\n",
       "      <td>0.811982</td>\n",
       "      <td>0.376536</td>\n",
       "      <td>-0.744661</td>\n",
       "      <td>1.146026</td>\n",
       "      <td>0.949427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15849</th>\n",
       "      <td>-1.242069</td>\n",
       "      <td>1.357780</td>\n",
       "      <td>1.142481</td>\n",
       "      <td>2.943035</td>\n",
       "      <td>-0.381281</td>\n",
       "      <td>0.164104</td>\n",
       "      <td>0.311346</td>\n",
       "      <td>0.138942</td>\n",
       "      <td>-0.938993</td>\n",
       "      <td>1.606443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114726</td>\n",
       "      <td>-0.006412</td>\n",
       "      <td>0.324604</td>\n",
       "      <td>0.521447</td>\n",
       "      <td>-0.630640</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>-0.483905</td>\n",
       "      <td>-0.146492</td>\n",
       "      <td>-0.171476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115543</th>\n",
       "      <td>1.060241</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>1.019393</td>\n",
       "      <td>2.582009</td>\n",
       "      <td>-0.304338</td>\n",
       "      <td>0.454251</td>\n",
       "      <td>-0.288715</td>\n",
       "      <td>0.325218</td>\n",
       "      <td>-0.408663</td>\n",
       "      <td>0.678529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210891</td>\n",
       "      <td>-0.538561</td>\n",
       "      <td>0.143262</td>\n",
       "      <td>0.165274</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>-0.176277</td>\n",
       "      <td>0.016360</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>-0.350191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68067</th>\n",
       "      <td>-1.101847</td>\n",
       "      <td>-1.632441</td>\n",
       "      <td>0.901067</td>\n",
       "      <td>0.847753</td>\n",
       "      <td>-1.249091</td>\n",
       "      <td>0.654937</td>\n",
       "      <td>1.448868</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>-0.136742</td>\n",
       "      <td>-0.150129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610654</td>\n",
       "      <td>0.835795</td>\n",
       "      <td>1.179955</td>\n",
       "      <td>-0.029091</td>\n",
       "      <td>-0.300896</td>\n",
       "      <td>0.699175</td>\n",
       "      <td>-0.336072</td>\n",
       "      <td>-0.177587</td>\n",
       "      <td>1.725376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220725</th>\n",
       "      <td>-1.169203</td>\n",
       "      <td>1.863414</td>\n",
       "      <td>-2.515135</td>\n",
       "      <td>5.463681</td>\n",
       "      <td>-0.297971</td>\n",
       "      <td>1.364918</td>\n",
       "      <td>0.759219</td>\n",
       "      <td>-0.118861</td>\n",
       "      <td>-2.293921</td>\n",
       "      <td>-0.423784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393090</td>\n",
       "      <td>-0.708692</td>\n",
       "      <td>0.471309</td>\n",
       "      <td>-0.078616</td>\n",
       "      <td>-0.544655</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>-0.240930</td>\n",
       "      <td>-0.781055</td>\n",
       "      <td>0.944509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150684</th>\n",
       "      <td>-10.040631</td>\n",
       "      <td>6.139183</td>\n",
       "      <td>-12.972972</td>\n",
       "      <td>7.740555</td>\n",
       "      <td>-8.684705</td>\n",
       "      <td>-3.837429</td>\n",
       "      <td>-11.907702</td>\n",
       "      <td>5.833273</td>\n",
       "      <td>-5.731054</td>\n",
       "      <td>-12.438945</td>\n",
       "      <td>...</td>\n",
       "      <td>2.823431</td>\n",
       "      <td>1.153005</td>\n",
       "      <td>-0.567343</td>\n",
       "      <td>0.843012</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>-0.307375</td>\n",
       "      <td>0.061631</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212354</th>\n",
       "      <td>-0.969974</td>\n",
       "      <td>1.515210</td>\n",
       "      <td>-0.620161</td>\n",
       "      <td>-0.497631</td>\n",
       "      <td>0.483776</td>\n",
       "      <td>-1.303317</td>\n",
       "      <td>0.875425</td>\n",
       "      <td>-0.110432</td>\n",
       "      <td>0.366743</td>\n",
       "      <td>0.603962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283907</td>\n",
       "      <td>1.398890</td>\n",
       "      <td>-0.138679</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>-0.619142</td>\n",
       "      <td>-0.235078</td>\n",
       "      <td>0.547562</td>\n",
       "      <td>0.242167</td>\n",
       "      <td>-0.350231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266783</th>\n",
       "      <td>0.140872</td>\n",
       "      <td>0.926565</td>\n",
       "      <td>-0.621609</td>\n",
       "      <td>-0.778584</td>\n",
       "      <td>1.179388</td>\n",
       "      <td>-0.221437</td>\n",
       "      <td>0.815257</td>\n",
       "      <td>0.106577</td>\n",
       "      <td>-0.174969</td>\n",
       "      <td>-0.645503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332997</td>\n",
       "      <td>-0.873519</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>-0.400130</td>\n",
       "      <td>0.130450</td>\n",
       "      <td>0.213862</td>\n",
       "      <td>0.067348</td>\n",
       "      <td>-0.342914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6971</th>\n",
       "      <td>-3.499108</td>\n",
       "      <td>0.258555</td>\n",
       "      <td>-4.489558</td>\n",
       "      <td>4.853894</td>\n",
       "      <td>-6.974522</td>\n",
       "      <td>3.628382</td>\n",
       "      <td>5.431271</td>\n",
       "      <td>-1.946734</td>\n",
       "      <td>-0.775680</td>\n",
       "      <td>-1.987773</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052368</td>\n",
       "      <td>0.204817</td>\n",
       "      <td>-2.119007</td>\n",
       "      <td>0.170279</td>\n",
       "      <td>-0.393844</td>\n",
       "      <td>0.296367</td>\n",
       "      <td>1.985913</td>\n",
       "      <td>-0.900452</td>\n",
       "      <td>6.882027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1        V2         V3        V4         V5        V6  \\\n",
       "106839   1.411190 -0.713597   0.386150 -1.007847  -0.577021  0.565564   \n",
       "72757   -2.986466 -0.000891   0.605887  0.338338   0.685448 -1.581954   \n",
       "155713  -0.513840  0.752265   2.375663 -0.104836   0.060729 -0.274978   \n",
       "10204   -4.641893  2.902086  -1.572939  2.507299  -0.871783 -1.040903   \n",
       "231978  -2.064240  2.629739  -0.748406  0.694992   0.418178  1.392520   \n",
       "27895    0.861545 -1.065624   0.672919  0.570590  -1.205622  0.270839   \n",
       "64436    1.593954 -1.041333  -1.593685 -2.524160   1.340290  2.947308   \n",
       "190368  -2.272473  2.935226  -4.871394  2.419012  -1.513022 -0.480625   \n",
       "57349   -0.334815  1.189703   1.748604  2.729949   0.291077  0.468765   \n",
       "200319   2.053859  0.017620  -1.044302  0.398776  -0.017421 -1.103648   \n",
       "150692 -11.320633  7.191950 -13.179083  9.099552 -10.094749 -2.440115   \n",
       "258056  -4.309441 -2.919986   1.405447  0.994070  -0.004181  1.006933   \n",
       "15849   -1.242069  1.357780   1.142481  2.943035  -0.381281  0.164104   \n",
       "115543   1.060241  0.294210   1.019393  2.582009  -0.304338  0.454251   \n",
       "68067   -1.101847 -1.632441   0.901067  0.847753  -1.249091  0.654937   \n",
       "220725  -1.169203  1.863414  -2.515135  5.463681  -0.297971  1.364918   \n",
       "150684 -10.040631  6.139183 -12.972972  7.740555  -8.684705 -3.837429   \n",
       "212354  -0.969974  1.515210  -0.620161 -0.497631   0.483776 -1.303317   \n",
       "266783   0.140872  0.926565  -0.621609 -0.778584   1.179388 -0.221437   \n",
       "6971    -3.499108  0.258555  -4.489558  4.853894  -6.974522  3.628382   \n",
       "\n",
       "               V7        V8        V9        V10  ...       V21       V22  \\\n",
       "106839  -1.062723  0.113954 -0.433717   0.536901  ...  0.332725  0.976893   \n",
       "72757    0.504206 -0.233403  0.636768   1.010291  ... -0.875146 -0.509849   \n",
       "155713   0.502985 -0.213632  1.615484  -1.132412  ... -0.306357 -0.467832   \n",
       "10204   -1.593901 -3.254905  1.908963   1.077418  ...  1.963597 -0.217414   \n",
       "231978  -1.697801 -6.333065  1.724184  -0.887242  ...  6.215514 -1.276909   \n",
       "27895   -0.673109  0.221099 -0.896328   0.910822  ... -0.065595 -0.155664   \n",
       "64436   -1.047326  0.614772 -2.322846   1.513246  ... -0.556379 -1.421794   \n",
       "190368  -2.126136  1.883507 -1.297262  -5.487425  ...  0.718504  0.893850   \n",
       "57349    0.478128  0.192521 -1.845558   0.830895  ...  0.338385  0.978178   \n",
       "200319   0.242713 -0.369792  0.378501   0.033398  ... -0.271011 -0.588222   \n",
       "150692 -14.184337  4.452503 -6.241960 -12.618163  ...  1.082235 -0.350563   \n",
       "258056   1.755872 -0.976941  1.846423   0.946266  ... -1.117336  0.011099   \n",
       "15849    0.311346  0.138942 -0.938993   1.606443  ... -0.114726 -0.006412   \n",
       "115543  -0.288715  0.325218 -0.408663   0.678529  ... -0.210891 -0.538561   \n",
       "68067    1.448868  0.023308 -0.136742  -0.150129  ...  0.610654  0.835795   \n",
       "220725   0.759219 -0.118861 -2.293921  -0.423784  ... -0.393090 -0.708692   \n",
       "150684 -11.907702  5.833273 -5.731054 -12.438945  ...  2.823431  1.153005   \n",
       "212354   0.875425 -0.110432  0.366743   0.603962  ...  0.283907  1.398890   \n",
       "266783   0.815257  0.106577 -0.174969  -0.645503  ... -0.332997 -0.873519   \n",
       "6971     5.431271 -1.946734 -0.775680  -1.987773  ... -1.052368  0.204817   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  \\\n",
       "106839 -0.264513 -1.248912  0.565982  0.062935  0.055746  0.011886   \n",
       "72757   1.313918  0.355065  0.448552  0.193490  1.214588 -0.013923   \n",
       "155713 -0.268527 -0.136685  0.229910 -0.722749 -0.084214 -0.147139   \n",
       "10204  -0.549340  0.645545 -0.354558 -0.611764 -3.908080 -0.671248   \n",
       "231978  0.459861 -1.051685  0.209178 -0.319859  0.015434 -0.050117   \n",
       "27895  -0.118910 -0.034928  0.180498 -0.303340  0.039835  0.054071   \n",
       "64436   0.113418  0.943166  0.499233 -0.434746  0.004902  0.009553   \n",
       "190368 -0.031632  0.322913 -0.058406 -0.411649  0.573803  0.176067   \n",
       "57349  -0.151153  0.262088 -0.326833  0.330151  0.151411  0.135550   \n",
       "200319  0.333976  0.015987 -0.277432  0.197468 -0.060623 -0.058174   \n",
       "150692  0.483044  0.661133 -0.396522 -0.413315 -0.997548 -0.235036   \n",
       "258056  1.111285  0.705787  0.811982  0.376536 -0.744661  1.146026   \n",
       "15849   0.324604  0.521447 -0.630640 -0.093659 -0.483905 -0.146492   \n",
       "115543  0.143262  0.165274  0.213565 -0.176277  0.016360  0.012227   \n",
       "68067   1.179955 -0.029091 -0.300896  0.699175 -0.336072 -0.177587   \n",
       "220725  0.471309 -0.078616 -0.544655  0.014777 -0.240930 -0.781055   \n",
       "150684 -0.567343  0.843012  0.549938  0.113892 -0.307375  0.061631   \n",
       "212354 -0.138679  0.053954 -0.619142 -0.235078  0.547562  0.242167   \n",
       "266783  0.022613 -0.029549 -0.400130  0.130450  0.213862  0.067348   \n",
       "6971   -2.119007  0.170279 -0.393844  0.296367  1.985913 -0.900452   \n",
       "\n",
       "        normalAmount  Class  \n",
       "106839     -0.265311      0  \n",
       "72757      -0.346073      1  \n",
       "155713     -0.308171      0  \n",
       "10204      -0.307691      1  \n",
       "231978     -0.321245      1  \n",
       "27895       0.422399      0  \n",
       "64436      -0.280184      0  \n",
       "190368      0.350034      1  \n",
       "57349      -0.277186      0  \n",
       "200319     -0.345313      0  \n",
       "150692     -0.201582      1  \n",
       "258056      0.949427      0  \n",
       "15849      -0.171476      0  \n",
       "115543     -0.350191      0  \n",
       "68067       1.725376      1  \n",
       "220725      0.944509      1  \n",
       "150684     -0.349231      1  \n",
       "212354     -0.350231      0  \n",
       "266783     -0.342914      0  \n",
       "6971        6.882027      1  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df  = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df= pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97cfc34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    492\n",
       "1    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a6ee8724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48816552",
   "metadata": {
    "id": "48816552"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ead22",
   "metadata": {
    "id": "2e6ead22"
   },
   "source": [
    "## 3. Class를 종속변수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb699ae0",
   "metadata": {
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1620908261886,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb699ae0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "X = df.values[:,0:29]\n",
    "Y = df.values[:,29]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ed960",
   "metadata": {},
   "source": [
    "## 4. Train Set : Test set = 85:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "70420ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6792f",
   "metadata": {},
   "source": [
    "## 5. ANN/DNN활용(Time Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3b005696",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 258196,
     "status": "error",
     "timestamp": 1620908671189,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "3b005696",
    "outputId": "50445b54-393e-45c0-b516-26a04c5d3fe5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 43ms/step - loss: 3.0744 - accuracy: 0.5128 - val_loss: 2.1626 - val_accuracy: 0.4921\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.16256, saving model to ./model\\01-2.1626,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\01-2.1626,hdf5\\assets\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9263 - accuracy: 0.5350 - val_loss: 1.4169 - val_accuracy: 0.4841\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.16256 to 1.41685, saving model to ./model\\02-1.4169,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\02-1.4169,hdf5\\assets\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4276 - accuracy: 0.5053 - val_loss: 0.8990 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41685 to 0.89898, saving model to ./model\\03-0.8990,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\03-0.8990,hdf5\\assets\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8392 - accuracy: 0.5490 - val_loss: 0.6638 - val_accuracy: 0.6508\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89898 to 0.66382, saving model to ./model\\04-0.6638,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\04-0.6638,hdf5\\assets\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6350 - accuracy: 0.7080 - val_loss: 0.5922 - val_accuracy: 0.8016\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66382 to 0.59225, saving model to ./model\\05-0.5922,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\05-0.5922,hdf5\\assets\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5530 - accuracy: 0.8564 - val_loss: 0.5264 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59225 to 0.52645, saving model to ./model\\06-0.5264,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\06-0.5264,hdf5\\assets\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4922 - accuracy: 0.8647 - val_loss: 0.4716 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52645 to 0.47160, saving model to ./model\\07-0.4716,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\07-0.4716,hdf5\\assets\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.8869 - val_loss: 0.4306 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47160 to 0.43062, saving model to ./model\\08-0.4306,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\08-0.4306,hdf5\\assets\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4128 - accuracy: 0.8820 - val_loss: 0.3982 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.43062 to 0.39816, saving model to ./model\\09-0.3982,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\09-0.3982,hdf5\\assets\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3775 - accuracy: 0.9111 - val_loss: 0.3698 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.39816 to 0.36979, saving model to ./model\\10-0.3698,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\10-0.3698,hdf5\\assets\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.9265 - val_loss: 0.3464 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36979 to 0.34644, saving model to ./model\\11-0.3464,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\11-0.3464,hdf5\\assets\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.9354 - val_loss: 0.3248 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.34644 to 0.32482, saving model to ./model\\12-0.3248,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\12-0.3248,hdf5\\assets\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2818 - accuracy: 0.9438 - val_loss: 0.3058 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32482 to 0.30577, saving model to ./model\\13-0.3058,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\13-0.3058,hdf5\\assets\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2866 - accuracy: 0.9267 - val_loss: 0.2896 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30577 to 0.28959, saving model to ./model\\14-0.2896,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\14-0.2896,hdf5\\assets\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2787 - accuracy: 0.9156 - val_loss: 0.2750 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.28959 to 0.27496, saving model to ./model\\15-0.2750,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\15-0.2750,hdf5\\assets\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2509 - accuracy: 0.9312 - val_loss: 0.2619 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27496 to 0.26192, saving model to ./model\\16-0.2619,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\16-0.2619,hdf5\\assets\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2231 - accuracy: 0.9487 - val_loss: 0.2503 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.26192 to 0.25030, saving model to ./model\\17-0.2503,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\17-0.2503,hdf5\\assets\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 0.9449 - val_loss: 0.2397 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.25030 to 0.23965, saving model to ./model\\18-0.2397,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\18-0.2397,hdf5\\assets\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2119 - accuracy: 0.9330 - val_loss: 0.2285 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.23965 to 0.22848, saving model to ./model\\19-0.2285,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\19-0.2285,hdf5\\assets\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9410 - val_loss: 0.2210 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.22848 to 0.22098, saving model to ./model\\20-0.2210,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\20-0.2210,hdf5\\assets\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9452 - val_loss: 0.2159 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22098 to 0.21591, saving model to ./model\\21-0.2159,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\21-0.2159,hdf5\\assets\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9528 - val_loss: 0.2106 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.21591 to 0.21059, saving model to ./model\\22-0.2106,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\22-0.2106,hdf5\\assets\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9450 - val_loss: 0.2097 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.21059 to 0.20967, saving model to ./model\\23-0.2097,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\23-0.2097,hdf5\\assets\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.2062 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.20967 to 0.20619, saving model to ./model\\24-0.2062,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\24-0.2062,hdf5\\assets\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.9464 - val_loss: 0.1987 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.20619 to 0.19869, saving model to ./model\\25-0.1987,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\25-0.1987,hdf5\\assets\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.9494 - val_loss: 0.1953 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.19869 to 0.19528, saving model to ./model\\26-0.1953,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\26-0.1953,hdf5\\assets\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9525 - val_loss: 0.1918 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.19528 to 0.19179, saving model to ./model\\27-0.1918,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\27-0.1918,hdf5\\assets\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9571 - val_loss: 0.1898 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.19179 to 0.18984, saving model to ./model\\28-0.1898,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\28-0.1898,hdf5\\assets\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.9449 - val_loss: 0.1851 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.18984 to 0.18511, saving model to ./model\\29-0.1851,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\29-0.1851,hdf5\\assets\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.9443 - val_loss: 0.1837 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.18511 to 0.18371, saving model to ./model\\30-0.1837,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\30-0.1837,hdf5\\assets\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1334 - accuracy: 0.9532 - val_loss: 0.1858 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.18371\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9520 - val_loss: 0.1849 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.18371\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9559 - val_loss: 0.1821 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.18371 to 0.18210, saving model to ./model\\33-0.1821,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\33-0.1821,hdf5\\assets\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1284 - accuracy: 0.9561 - val_loss: 0.1781 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.18210 to 0.17809, saving model to ./model\\34-0.1781,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\34-0.1781,hdf5\\assets\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9625 - val_loss: 0.1816 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.17809\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9559 - val_loss: 0.1846 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.17809\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9587 - val_loss: 0.1785 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.17809\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1095 - accuracy: 0.9615 - val_loss: 0.1695 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.17809 to 0.16952, saving model to ./model\\38-0.1695,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\38-0.1695,hdf5\\assets\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9514 - val_loss: 0.1674 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.16952 to 0.16744, saving model to ./model\\39-0.1674,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\39-0.1674,hdf5\\assets\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9600 - val_loss: 0.1701 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.16744\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9646 - val_loss: 0.1714 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.16744\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9618 - val_loss: 0.1726 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.16744\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9671 - val_loss: 0.1703 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.16744\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9630 - val_loss: 0.1634 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.16744 to 0.16340, saving model to ./model\\44-0.1634,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\44-0.1634,hdf5\\assets\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9700 - val_loss: 0.1619 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.16340 to 0.16192, saving model to ./model\\45-0.1619,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\45-0.1619,hdf5\\assets\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9683 - val_loss: 0.1566 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.16192 to 0.15659, saving model to ./model\\46-0.1566,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\46-0.1566,hdf5\\assets\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9736 - val_loss: 0.1536 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.15659 to 0.15360, saving model to ./model\\47-0.1536,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\47-0.1536,hdf5\\assets\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0892 - accuracy: 0.9816 - val_loss: 0.1521 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.15360 to 0.15208, saving model to ./model\\48-0.1521,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\48-0.1521,hdf5\\assets\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0955 - accuracy: 0.9767 - val_loss: 0.1530 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15208\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9739 - val_loss: 0.1569 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15208\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9701 - val_loss: 0.1605 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15208\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9719 - val_loss: 0.1632 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15208\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9665 - val_loss: 0.1616 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15208\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9750 - val_loss: 0.1635 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15208\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1115 - accuracy: 0.9608 - val_loss: 0.1636 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.15208\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9774 - val_loss: 0.1637 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.15208\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9757 - val_loss: 0.1639 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.15208\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9746 - val_loss: 0.1643 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15208\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9808 - val_loss: 0.1631 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.15208\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0874 - accuracy: 0.9786 - val_loss: 0.1633 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.15208\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0833 - accuracy: 0.9769 - val_loss: 0.1600 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.15208\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9719 - val_loss: 0.1570 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.15208\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9800 - val_loss: 0.1580 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.15208\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.9814 - val_loss: 0.1606 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.15208\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9782 - val_loss: 0.1570 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.15208\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9722 - val_loss: 0.1623 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.15208\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0673 - accuracy: 0.9827 - val_loss: 0.1648 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.15208\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.9757 - val_loss: 0.1625 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.15208\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9810 - val_loss: 0.1627 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.15208\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9768 - val_loss: 0.1599 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.15208\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9792 - val_loss: 0.1621 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.15208\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9748 - val_loss: 0.1636 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.15208\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9776 - val_loss: 0.1657 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.15208\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9778 - val_loss: 0.1664 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.15208\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9828 - val_loss: 0.1682 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.15208\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9778 - val_loss: 0.1672 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.15208\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9802 - val_loss: 0.1692 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.15208\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.9871 - val_loss: 0.1707 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.15208\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9824 - val_loss: 0.1692 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.15208\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9794 - val_loss: 0.1680 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.15208\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9797 - val_loss: 0.1664 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.15208\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9792 - val_loss: 0.1655 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.15208\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9805 - val_loss: 0.1682 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.15208\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9857 - val_loss: 0.1660 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.15208\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.9820 - val_loss: 0.1675 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.15208\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0553 - accuracy: 0.9877 - val_loss: 0.1736 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.15208\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9864 - val_loss: 0.1732 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.15208\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9813 - val_loss: 0.1702 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.15208\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9885 - val_loss: 0.1689 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.15208\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9847 - val_loss: 0.1700 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.15208\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.9905 - val_loss: 0.1658 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.15208\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.9852 - val_loss: 0.1662 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.15208\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9879 - val_loss: 0.1727 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.15208\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9859 - val_loss: 0.1771 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.15208\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0467 - accuracy: 0.9892 - val_loss: 0.1796 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.15208\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9918 - val_loss: 0.1795 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.15208\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9888 - val_loss: 0.1755 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.15208\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9841 - val_loss: 0.1733 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.15208\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9905 - val_loss: 0.1776 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.15208\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9863 - val_loss: 0.1744 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.15208\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9925 - val_loss: 0.1773 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.15208\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9874 - val_loss: 0.1860 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.15208\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0401 - accuracy: 0.9916 - val_loss: 0.1838 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.15208\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9909 - val_loss: 0.1795 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.15208\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9934 - val_loss: 0.1787 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.15208\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9908 - val_loss: 0.1774 - val_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss did not improve from 0.15208\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9886 - val_loss: 0.1774 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.15208\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9916 - val_loss: 0.1801 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.15208\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9933 - val_loss: 0.1835 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.15208\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.9912 - val_loss: 0.1832 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.15208\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9934 - val_loss: 0.1853 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.15208\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.9884 - val_loss: 0.1812 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.15208\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9939 - val_loss: 0.1843 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.15208\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9917 - val_loss: 0.1863 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.15208\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9905 - val_loss: 0.1864 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.15208\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9947 - val_loss: 0.1898 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.15208\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9936 - val_loss: 0.1911 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.15208\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9906 - val_loss: 0.1929 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.15208\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9930 - val_loss: 0.1947 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.15208\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9936 - val_loss: 0.1961 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.15208\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9950 - val_loss: 0.2012 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.15208\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 0.2032 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.15208\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9908 - val_loss: 0.2008 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.15208\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9907 - val_loss: 0.2022 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.15208\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.9918 - val_loss: 0.2023 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.15208\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9954 - val_loss: 0.2032 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.15208\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9943 - val_loss: 0.2105 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.15208\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9928 - val_loss: 0.2118 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.15208\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.9968 - val_loss: 0.2096 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.15208\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9955 - val_loss: 0.2134 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.15208\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9971 - val_loss: 0.2097 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.15208\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.9944 - val_loss: 0.2117 - val_accuracy: 0.9365\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.15208\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0300 - accuracy: 0.9932 - val_loss: 0.2173 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.15208\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 1.00 - 0s 4ms/step - loss: 0.0272 - accuracy: 0.9954 - val_loss: 0.2321 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.15208\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9936 - val_loss: 0.2327 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.15208\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9934 - val_loss: 0.2281 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.15208\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 0.2282 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.15208\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.9975 - val_loss: 0.2276 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.15208\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9959 - val_loss: 0.2305 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.15208\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9972 - val_loss: 0.2314 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.15208\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9962 - val_loss: 0.2310 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.15208\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.2342 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.15208\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.2329 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.15208\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 0.9951 - val_loss: 0.2351 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.15208\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.2406 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.15208\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.9939 - val_loss: 0.2422 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.15208\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 0.2428 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.15208\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9957 - val_loss: 0.2424 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.15208\n",
      "테스트 손실값 : 0.1505945771932602, 테스트 정확도 : 0.9594594836235046\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=29, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "model_path='./model/{epoch:02d}-{val_loss:.4f},hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 조기 종료\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 200, batch_size = 100, callbacks=[early_stopping_callback,checkpointer])\n",
    "score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "\n",
    "print(f'테스트 손실값 : {score[0]}, 테스트 정확도 : {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eda27de9",
   "metadata": {
    "id": "eda27de9",
    "outputId": "ddc1df91-57b2-410d-bb07-2719282bc855",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 30)                900       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 18)                558       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,699\n",
      "Trainable params: 1,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "951761ad",
   "metadata": {
    "id": "951761ad",
    "outputId": "fda90a61-9164-4819-c6ff-99e812b3704f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "# 딕셔너리 형태\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5778e887",
   "metadata": {
    "id": "5778e887",
    "outputId": "bc990525-34af-48c5-ed3f-fbfa43e57480",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "y_value = history.history['val_loss']  # 오차값을 저장\n",
    "y_acc = history.history['accuracy']    # 정밀도를 저장\n",
    "\n",
    "print(len(y_value))\n",
    "print(len(y_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d143a39",
   "metadata": {},
   "source": [
    "### * **오차,정확도 그래프**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "95774024",
   "metadata": {
    "id": "95774024",
    "outputId": "143b0849-9f58-49ec-9997-e99a7621da49"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_value, c='red', markersize=3)\n",
    "plt.plot(x_len, y_acc, c='blue', markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c57a4",
   "metadata": {},
   "source": [
    "### * **Class=1값 정확도 검증**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "900f504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_one = df[\"Class\"] == 1\n",
    "all_class_1 = df[class_one]\n",
    "\n",
    "all_class_1.drop('Class', axis=1, inplace = True)\n",
    "\n",
    "dt_one = pd.DataFrame(model.predict(all_class_1))\n",
    "plt.hist(dt_one, bins=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "module5_time_정규화_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
